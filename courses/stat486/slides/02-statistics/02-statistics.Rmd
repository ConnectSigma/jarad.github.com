---
layout: page
title: STAT 486/586
tagline: "Review - Statistics"
author: "Jarad Niemi"
date: "`r Sys.Date()`"
output: 
  html_document:
      toc: true
      toc_float: true
---
{% include JB/setup %}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Statistics

Statistics can generally be broken up into exploratory statistics and 
inferential statistics. 
Exploratory statistics is generally what most of the population thinks about 
when you say statistics, e.g. means, medians, figures, etc. 

## Exploratory statistics

When we collect data, 
we should always perform some exploratory analysis on it. 
The type of exploratory analysis we perform will depend on the type of data 
obtained. 
Generally, exploratory statistics can be categorized into descriptive and
graphical statistics. 


### Descriptive statistics

Descriptive statistics are utilized when you want to numerically quantify some
aspect of your data. 
If your data are categorical, then you are probably counting the number of 
observations in each category.
If your data are continuous, then you are typically calculating some 
*sample* statistics.

- measures of location
  - min/max
  - mean
  - median
- measures of spread
  - range
  - interquartile range
  - standard deviation
  
These sample statistics provide a concise description of the data, 
but frankly are quite lacking for any type of understanding.


### Graphical statistics

Much more powerful than descriptive statistics are graphical statistics that
provides summaries of data in the form of graphics or figures. 
Some common graphical statistics are

- pie charts [I'm screaming inside]
- bar charts [I'm still screaming inside]
- histograms [I'm calming down...but just a little]
- boxplots [here we go again]
- scatterplots [whew...I'm relaxed]

Constructing useful graphical statistics is a widely under-utilized skill!!
If you can convey relationships in data using graphics, 
then you don't need inferential statistics
(regardless of what your statistics professors tell you). 

## Inferential statistics

Inferential statistics encompasses most of what you have probably learned in 
your prerequisite statistics courses including p-values, hypothesis tests, and
confidence intervals. 

But lost in all of those formulas are the two most important concepts in making
*inferences*:

- to infer about a population, you need a *random sample* from that population
- to infer cause-and-effect, you must *randomly assign* the treatment

When the sample is not a random sample,
then all your inferential statistics are just convenient summaries of the data,
but say nothing about the population you intend to study. 

When the treatment is not randomly assigned,
then you only have an association and not a causation.

Thus there is a limit to what statistics can do with a particular data set,
based on how that data were obtained.

Now...on with the formulas.


### Binomial

If we have binomial data, then we are typically interested in understanding 
the probability of success. 
(There are really interesting problems where you don't know the number of 
attempts, e.g. estimating population sizes.)

#### Maximum likelihood estimator

If $Y\sim Bin(n,\theta)$, 
then the likelihood for $\theta$ is 
$$L(\theta) = {n\choose y}\theta^y(1-\theta)^{n-y}, \qquad 0<\theta<1$$.
The maximum likelihood estimator for $\theta$ is 
$$\hat\theta_{MLE} = y/n$$
as this is the value that maximizes the likelihood $L(\theta)$. 

#### Confidence interval

A 100(1-a)\% (approximate) confidence interval for $\theta$ based on
$y$ successes out of $n$ attempts is 
$$\hat\theta \pm z_{a/2}\sqrt{\hat\theta(1-\hat\theta)}$$
where $\hat\theta = y/n$ and $z_{a/2}$ is the z-critical value such that 
$$a/2 = \int_{z_{a/2}}^\infty \phi(x) dx$$ where $\phi(x)$ is the probability
density function for a standard normal. 
This formula is based on an asymptotic (sample size
going to infinity) argument based on the CLT. 

This is one formula, but [there are a whole bunch of others](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval). 
So, even in the simplest possible models, things are not so clear.
Please note the [Bayesian approach](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Jeffreys_interval).

#### Hypothesis test

To conduct a hypothesis test with null hypothesis that the true probability of 
success is $\theta_0$, we calculate 
$$z = \frac{\hat\theta-\theta_0}{\sqrt{\frac{\theta_0(1-\theta_0)}{n}}}.$$
When $z$ is sufficiently large, this has an approximate standard normal and we
can calculate a two-sided $p$-value
$$p\mbox{-value} = 2P(Z < -|z|).$$
If $p$-value is less than our significance level $a$, then we *reject the null model* and, otherwise, we *fail to reject the null model*.
The *null model* is 
$$Y \sim Bin(n,\theta_0).$$
What does this mean? It means there is evidence against this model, but that
could mean 

- the attempts are not independent
- the attempts don't have a common probability
- the common probability is not $\theta_0$
- the number of attempts is not $n$

The result of the test does not tell us which of these is the problem. 
In addition, even if everything is true except (possibly) the common probability
$\theta_0$. 
Our interpretation of the $p$-value requires more context about the scientific
setup. 

I highly recommend every one read the 
[ASA's Statement on $p$-Values](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108). 
Then reread it periodically until it makes some sense. 


### Normal

If we collect data that are normally distributed, 
we typically don't know the mean or the variance.
Thus we have $Y_i \stackrel{ind}{\sim} N(\mu,\sigma^2)$. 
Our goal is often to make statements about $\mu$,
but we may also be interested in $\sigma$. 

#### Maximum likelihood estimator

The likelihood for $\mu$ and $\sigma^2$ is 
$$L(\mu,\sigma^2) = \prod_{i=1}^n (2\pi \sigma^2)^{-1/2}\exp\left(-\frac{1}{2\sigma^2}(y_i-\mu)^2\right)=
(2\pi \sigma^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\mu)^2\right).$$
The maximum likelihood estimator for $\mu$ and $\sigma^2$ is 
$$\hat\mu_{MLE} = \overline{y} 
\qquad \mbox{and} \qquad
\hat\sigma^2=\frac{1}{n}\sum_{i=1}^n (y_i-\overline{y})^2 = \frac{n-1}{n}s^2$$
where $\overline{y}$ is the sample mean and $s^2$ is the sample variance. 

#### Confidence interval for $\mu$

A 100(1-a)\% confidence interval for $\mu$ is 
$$\overline{y} \pm t_{n-1,a/2} s/\sqrt{n}$$
where $t_{n-1,a/2}$ is the $t$-critical value using the Student's $T$-distribution with
$n-1$ degrees of freedom. 

#### Hypothesis test

A hypothesis test with null hypothesis $\mu=\mu_0$ calculates the $t$-statistic
$$t = \frac{\overline{y}-\mu_0}{s/\sqrt{n}}.$$
A $p$-value is calculated using 
$$p\mbox{-value} = 2P(T_{n-1} < -|t|)$$
where $T_{n-1}$ is a Student's $T$-distribution with $n-1$ degrees if freedom.

If $p$-value is less than our significance level $\alpha$, 
we *reject the null model*. 
Otherwise, we *fail to reject the null model*.
All the same caveats about hypothesis tests mentioned above apply. 

Read the damn [ASA Statement on $p$-Values](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)!



### Simple linear regression

The simple linear regression (SLR) models expands the applicability of normal based
modeling be shifting the mean according to another variable.
I use the terms *response variable* ($Y$) and *explanatory variable* ($X$) to  
identify the variable that explains the change in the response variable. 

To run a simple linear regression model, 
we need to record both the response and explanatory variable for each 
observation, 
i.e. we have a double $(Y_i,X_i)$ for all $i=1,\ldots,n$.
Then our model is 
$$Y_i \stackrel{ind}{\sim} N(\beta_0+\beta_1X_i,\sigma^2).$$
Although this is a succinct way of writing the model, 
I think a more intuitive way to write the model is 
$$Y_i = \beta_0+\beta_1X_i+\epsilon_i, \qquad \epsilon_i\stackrel{ind}{\sim} N(0,\sigma^2).$$
This notation more directly provides the 4 assumptions in a simple linear 
regression model:

- independent errors
- normal errors
- errors have constant variance
- linear relationship between explanatory variable (X) and the mean of the response variable

The last assumption is most straight-forward when you write
$$E[Y_i] = \beta_0+\beta_1X_i.$$

Now we could go through all the formulas related with the SLR model,
but suffice it to say that we can construct confidence intervals and 
hypothesis tests for any $\beta$ and these involve the $T$-distribution. 
Software generally defaults to providing hypothesis tests relative to the 
$\beta$s being 0. 
This makes sense for $\beta_1$ since, if it is 0, then that means the 
explanatory variable has no effect on the response variable. 

All the usual caveats apply for hypothesis tests here. 
Have you read the [ASA Statement on $p$-Values](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)
yet?


#### Interpretation

The most useful aspect of SLR models is the simplicity of interpretation:

> For each unit increase in the explanatory variable, the mean of the response variable increases by $\beta$.


### Multiple linear regression


Multiple (linear) regression is the extension of the SLR model to more than 
one continuous or binary explanatory variable. 
It also includes quadratic terms as well as interactions. 
Now, for each observation, you need to collect the value of the response 
variable as well as all of the explanatory variables, i.e.
$(Y_i,X_{i,1},\ldots,X_{i,p})$ for all $i=1,\ldots,n$. 
(I'm thinking spreadsheets might be pretty handy about now.)


The model can be written 
$$Y_i = \beta_0+\beta_1X_{i,p}+\ldots+\beta_p X_{i,p}+\epsilon_i, \qquad \epsilon_i\stackrel{ind}{\sim} N(0,\sigma^2).$$
But, it is much more succinct when you use some linear algebra
$$Y = X\beta + e, \quad e \sim N(0,\sigma^2\mathrm{I})$$
but now we have to define 

- vector $Y = (Y_1,\ldots,Y_n)^{\top}$,
- vector $\beta = (\beta_1,\ldots,\beta_p)$, 
- vector errors $\epsilon = (\epsilon_1,\ldots,\epsilon_n)$,
- model matrix $X$ which is $n \times p$ with each row being the 
- vector $X_i = (X_{i,1},\ldots,X_{i,p})$. 

And the models aren't quite identical since the matrix version does not have
an explicit intercept, but instead you can include an intercept by having the
first column of X be all 1s. 


#### Maximum likelihood estimator

Using some linear algebra and assuming we have a full rank $X$, 
the maximum likelihood estimator for $\beta$ is nice and succinct 
$$\hat\beta_{MLE} = (X^\top X)^{-1}X^\top y.$$
How pretty is that!!
(Break this out at parties and impress all your friends.
Don't write it down, just say it.)

#### Interpretation

Multiple regression models are much better at representing reality than 
simple linear regression models 
(because, generally, there is more than one explanatory variable affecting the response variable. )
With this flexibility, 
we do lose some interpretability. 
If there are no interactions, 
then we can interpret the coefficient for the $j$th explantory variable like this

> holding all other variables constant, a one unit change in the $j$th explanatory variable increases the mean of the response variable by $\beta_j$.

When you read about this in the newspaper or a journal article,
they will often use the phrase "after adjusting for ..." 
or "after controlling for ...". 

When interactions are present, everything becomes much more complicated. 
Are you ready to use figures yet?


