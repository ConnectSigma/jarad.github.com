---
title: "Regularization"
author: "Jarad Niemi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
layout: page
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  eval = TRUE)
```

[R code](28-regularization.R)

```{r packages}
library("tidyverse"); theme_set(theme_bw())
library("MASS")
library("glmnet")
library("knitr")
library("kableExtra")
```


# Variable selection

In regression models with many explanatory variables, 
there is a question of which explanatory variables should be included in a 
model. 
If, ultimately, we choose a single set, this is a 
_model_ or _variable selection_ problem. 

For this and future analyses in these slides, 
I will use a subset of the diamonds data set where we randomly select
100 observations and eliminate (for simplicity) the categorical variables. 

```{r}
set.seed(20230425)
n <- 100
d <- diamonds %>%
  dplyr::select(-cut, -color, -clarity) %>%
  rename(lprice = price) %>%
  mutate(lprice = log(lprice))

train <- d %>% sample_n(n)
test  <- d %>% sample_n(n)
```

We already know we can formally test two models that are nested. 

```{r}
m1 <- lm(lprice ~ .,   data = train)
m2 <- lm(lprice ~ .^2, data = train)
anova(m1, m2)
```
We can utilize this approach to search through models sequentially to 
determine which explanatory variables (and interactions) should be 
included in the final model. 

R has a `step()` function that will automatically perform this process,
although rather than utilizing the F-test and associated p-value it will use
Akaike's Information Criterion (AIC). 

The general AIC formula is -2 times the log likelihood plus two times the 
number of $\beta$s (call this $p$). 
For linear regression the formula is 
\[
AIC: -\log(\hat\sigma^2) + 2p
\]
The $2p$ is a penalty that attempts to reduce the number of parameters and,
therefore explanatory variables, in the model. 
Since we are trying to minimize $\sigma^2$ and minimize the number of parameters
we are looking for models whose AIC is small. 

An alternative to AIC is BIC which replaces the penalty by $\log(n)\times p$. 
Since $\log(n)>2$ for $n > 7$, BIC generally suggests smaller models than 
AIC. 


## Forward

One approach is to start with the most basic model:
one that only includes the intercept. 
Then try adding variables that improve (decrease) AIC. 

```{r}
m_forward <- step(lm(lprice ~ 1, data = train), 
          scope = formula(lm(lprice ~ .^2, data = train)), 
          direction = "forward")
```

Final model

```{r}
summary(m_forward)$coef
```

Training and testing error

```{r}
p <- predict(m_forward, newdata = test)

error <- data.frame(
  method = "Forward selection",
  in_sample = mean(m_forward$residuals^2),
  out_of_sample = mean((p-test$lprice)^2)
) 
```


## Backward

Another approach is to start with the largest model and eliminate variables
that (when eliminated) decrease AIC. 

```{r}
m_backward <- step(lm(lprice ~ .^2, data = train), 
          direction = "backward")
```

Final model

```{r}
summary(m_backward)$coef
```

Training and testing error

```{r}
p <- predict(m_backward, newdata = test)

error <- bind_rows(
  error,
  data.frame(
    method = "Backward selection",
    in_sample = mean(m_backward$residuals^2),
    out_of_sample = mean((p-test$lprice)^2)
  ) 
)
```


## Forward and backward

Or we can traverse in both directions starting from somewhere

```{r}
m_both <- step(lm(lprice ~ ., data = train), 
          scope = formula(lm(lprice ~ .^2, data = train)), 
          direction = "both")
```

Final model

```{r}
summary(m_both)$coef
```
As can be seen from this example, there is no reason that these approaches
will lead to the same model.


Training and testing error

```{r}
p <- predict(m_both, newdata = test)

error <- bind_rows(
  error,
  data.frame(
    method = "Forward and backward",
    in_sample = mean(m_both$residuals^2),
    out_of_sample = mean((p-test$lprice)^2)
  ) 
)
```



## Model averaging

Rather than selecting a single model, 
we can utilize all models and give each model a probability. 
Then, for each test observation, we can average our predictions across all the 
models. 

```{r}
m_avg <- lm(lprice ~ ., data = train, na.action = na.fail) %>%
  MuMIn::dredge() 
```

Obtain predictions. 

```{r}
p <- m_avg %>%
  get.models(subset = cumsum(weight) <= .99) %>%
  model.avg() %>%
  predict(newdata = test)

error <- bind_rows(
  error,
  data.frame(
    method = "Model averaging",
    in_sample = NA,
    out_of_sample = mean((p-test$lprice)^2)
  ) 
)
```




# Regression

Recall that a multiple regression model can be written 
\[
Y = X\beta + \epsilon, \quad \epsilon \sim N(0,\sigma^2,\mathrm{I}_n)
\]
where 

- $Y$ is $n\times 1$ vector of response variable values,
- $X$ is $n\times p$ matrix of explanatory variable values,
- $\beta$ is $p\times 1$ coefficient vector, and
- $\mathrm{I}_n$ is an $n\times n$ identity matrix. 


The MLE for $\beta$ is 
\[ 
\hat\beta_{MLE} = [X^\top X]^{-1} X^\top y.
\]

This MLE is the solution to the formula 
\[
\hat\beta_{MLE} = \mbox{argmin}_{\beta}\, (Y-X\beta)^\top (Y-X\beta),
\]
i.e. the sum of squared residuals. 


# Ridge regression

For ridge regression, 
we add a _penalty_ to the formula above. 
Specifically, the ridge regression estimator is the solution to 
\[
\hat\beta_{ridge} = \mbox{argmin}_{\beta}\, (Y-X\beta)^\top (Y-X\beta) + \lambda \sum_{j} \beta_j^2
\]
where the penalty is $\lambda \sum_{j} \beta_j^2$ for some value of $\lambda>0$.
Since we are trying to find the minimum and this penalty sums the square of the
$\beta$s, 
the solution to this formula will have $\beta$ closer to zero.
Thus, this method is one of the _shrinkage methods_. 

The solution is available in closed form as 
\[
\hat\beta_{ridge} = [X^\top X + \lambda\mathrm{I}_p]^{-1} X^\top y.
\]

## lm.ridge

We can fit a ridge regression model using the `MASS::lm.ridge()` function. 

Let's utilize the `diamonds` data set, but, for simplicity, only include the
continuous variables.



Fit ridge regression

```{r}
m <- MASS::lm.ridge(lprice ~ ., data = train, lambda = seq(0, .1, .001))
```

### Centering and scaling

For computational reasons and comparability reasons, 
the explanatory variables have been centered and scaled. 
Here are the values used for the centering and scaling:

```{r}
m$xm     # center
m$scales # scaling
```

We can verify that this centering and scaling are related to the mean and 
standard deviation of the explanatory variables. 

```{r}
apply(train, 2, mean)
apply(train, 2, sd) * sqrt((n-1) / n)
```

### Shrinking

The parameter estimates shrink toward zero as the ridge penalty increase as 
can be seen in this plot.

```{r}
plot(m)
```

Estimation of a ridge regression model involves choosing a value for $\lambda$. 

```{r}
select(m)
```
Here GCV stands for generalized cross validation. 

Refit the model with a chosen value for $\lambda$. 

```{r}
m <- MASS::lm.ridge(lprice ~ ., data = train, lambda = 0.035)
```

To make predictions, we need to center and scale our explanatory variables

```{r}
xtest <- scale(subset(test, select=-c(lprice)), center = m$xm, scale = m$scales)
```

```{r}
p <- xtest %*% m$coef +   # X*beta
  m$ym                    # need to include mean
```

```{r}
plot(p, test$lprice)
abline(0,1, col='red')
```

Since the predicted values and observed values following the y=x line, 
using R2 is a reasonable statistic. 

```{r}
RIDGE_R2   <- cor(p, test$lprice)^2 
RIDGE_RMSE <- mean((test$lprice - p)^2)
```


# LASSO

For least absolute shrinkage and selection operator (LASSO), 
we add a _penalty_ to the lm formula above. 
Specifically, the LASSO estimate is the solution to 
\[
\hat\beta_{LASSO} = \mbox{argmin}_{\beta}\, (Y-X\beta)^\top (Y-X\beta) + \lambda \sum_{j} |\beta_j|
\]
where the penalty is $\lambda \sum_{j} |\beta_j|$ for some value of $\lambda>0$.
Since we are trying to find the minimum and this penalty sums the square of the
$\beta$s, 
the solution to this formula will have $\beta$ closer to zero.
Thus, this method is one of the _shrinkage methods_. 

The penalty here is very similar to the ridge penalty, but here we use the 
absolute value rather than the squared value of the coefficient. 



