---
title: "Random Forests"
author: "Jarad Niemi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
layout: page
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE
)
```

[R code](29-random_forests.R)

```{r packages}
library("tidyverse")
theme_set(theme_bw())
library("rpart")
library("rpart.plot")
library("glmnet")
library("DT")
```

In these slides, we'll introduce classification and regression trees
(CART). 

For simplicity in the analyses, 
I will use a subset of the diamonds data set where we randomly select
100 observations and eliminate (for simplicity) the categorical variables. 

```{r}
set.seed(20230425) # This matches what was used in a previous set of slides
n <- 100
d <- diamonds %>%
  dplyr::select(-cut, -color, -clarity) %>%
  rename(lprice = price) %>%
  mutate(lprice = log(lprice))

train <- d %>% sample_n(n)
test <- d %>% sample_n(n)
```

Load up previous error file

```{r}
error <- read_csv("../28-penalty/error.csv")
```


# Classification and Regression Trees (CART)

```{r}
m <- rpart(lprice ~ ., data = train)
```

## Tree

Trees (in the real world) are constructed of a trunk, branches, and leaves. 
CART trees utilize the same structure where the trunk is composed of all 
observations, the branches split those observations, and (eventually) 
result in leaves. 

These leaves are composed of a collection of observations. 
The mean of those observations is the estimated and predicted mean for any
observations that fall into that leaf. 

The plot below provides the dummy variables that split the observations. 
The top number in each box is the mean of those observations.
The percentage in the each box is the percentage of all observations that 
end up in that branch or leaf. 

```{r}
rpart.plot(m, uniform = TRUE)
```

This regression tree utilizes only the explanatory variables carat, x, and z. 
These variables are related to the size of the diamond:
carat is weight, x is depth, and z is height. 
Thus, this tree is ordered from left (smallest) to right (largest) diamonds and 
the log price increases from left to right. 

### Regression model

Recall that a regression model is specified by a set of explanatory variables
$X_1,\ldots,X_p$. Here those explanatory variables are the product of 
dummy variables that lead to a leaf. 
Based on the tree above, the explanatory variables are

- $X_1 = \mathrm{I}(x < 5.7)\mathrm{I}(z < 3.1)\mathrm{I}(x < 4.4)$
- $X_2 = \mathrm{I}(x < 5.7)\mathrm{I}(z < 3.1)\mathrm{I}(x \ge 4.4)$
- $X_3 = \mathrm{I}(x < 5.7)\mathrm{I}(z \ge 3.1)$
- $X_4 = \mathrm{I}(x \ge 5.7)\mathrm{I}(carat < 1.2)\mathrm{I}(x < 6.1)$
- $X_5 = \mathrm{I}(x \ge 5.7)\mathrm{I}(carat < 1.2)\mathrm{I}(x \ge 6.1)$
- $X_6 = \mathrm{I}(x \ge 5.7)\mathrm{I}(carat \ge 1.2)\mathrm{I}(x < 7.3)$
- $X_7 = \mathrm{I}(x \ge 5.7)\mathrm{I}(carat \ge 1.2)\mathrm{I}(x \ge 7.3)$

Recall that for these data `lprice` is our response. 

Some of these dummy variables can be simplified, e.g. 

- $X_1 = \mathrm{I}(z < 3.1)\mathrm{I}(x < 4.4)$

The resulting regression model has 
\[ 
E[Y_i] = \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5 + \beta_6X_6
\]
where $\beta_j$ is the mean for the observations with $X_j = 1$. 


## Iterative construction

We can take a look at the iterative construction of the model using

```{r, include=FALSE}
summary(m)
```


## Predictions

```{r}
p <- bind_rows(
  test  %>% mutate(p = predict(m, newdata = test),  type = "test"),
  train %>% mutate(p = predict(m, newdata = train), type = "train")
)

ggplot(p, aes(x = p, y = lprice, shape = type, color = type)) + 
  geom_abline(intercept = 0, slope = 1, color = "gray") + 
  geom_point(position = position_dodge(width = 0.1)) 
```

```{r}
p_train <- predict(m, newdata = train)
p_test  <- predict(m, newdata = test)

error <- bind_rows(
  error,
  data.frame(
    group         = "CART",
    method        = "default",
    in_sample     = mean((p_train - train$lprice)^2),
    out_of_sample = mean((p_test  -  test$lprice)^2)
  )
)
```

## Tuning parameters

```{r}
args(rpart.control)
```
Each argument can tune the CART model to underfit or overfit. 
For example, 

- minsplit
    - low values lead to overfitting
    - high values lead to underfitting
- minbucket
    - low values lead to overfitting
    - high values lead to underfitting
- cp
    - low values lead to overfitting
    - high values lead to underfitting
    
    
### Underfit

```{r}
m <- rpart(lprice ~ ., data = train,
           control = rpart.control(
             minsplit = 40,
             minbucket = 20,
             cp = 0.1
           ))
```

```{r}
rpart.plot(m)
```

```{r}
p_train <- predict(m, newdata = train)
p_test  <- predict(m, newdata = test)

error <- bind_rows(
  error,
  data.frame(
    group         = "CART",
    method        = "underfit",
    in_sample     = mean((p_train - train$lprice)^2),
    out_of_sample = mean((p_test  -  test$lprice)^2)
  )
)
```

### Overfit

```{r}
m <- rpart(lprice ~ ., data = train,
           control = rpart.control(
             minsplit = 10,
             minbucket = 5,
             cp = 0.001
           ))
```

```{r}
rpart.plot(m)
```



```{r}
p_train <- predict(m, newdata = train)
p_test  <- predict(m, newdata = test)

error <- bind_rows(
  error,
  data.frame(
    group         = "CART",
    method        = "overfit",
    in_sample     = mean((p_train - train$lprice)^2),
    out_of_sample = mean((p_test  -  test$lprice)^2)
  )
)
```






# Random forests



# Summary

```{r}
error %>%
  datatable(
    rownames = FALSE,
    caption = "In and out-of-sample error for various prediction methods",
    filter = "top"
  ) %>%
  formatRound(columns = c("in_sample","out_of_sample"), digits = 3)
```