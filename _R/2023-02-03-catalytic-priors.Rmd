---
layout: post
title: "Catalytic priors"
description: ""
category: [Bayesian]
tags: [Bayesian, prior]
author: "Jarad Niemi"
date: "`r Sys.Date()`"
output: 
  html_document:
      toc: true
      toc_float: true
---

{% include JB/setup %}

This post was created to aid discussion in the Bayesian Working Group at 
Iowa State University concerning *catalytic priors*. 
The starting point for discussion is the manuscript
[Catalytic prior distributions with application to genearlized linear models]
(https://www.pnas.org/doi/abs/10.1073/pnas.1920913117) by
Huang, Stein, Rubin, and Kou. 
This is really my first foray into this area, so we'll see how well it goes.

# Catalytic priors

The basic idea of catalytic priors is that you construct a prior by 

1. fitting a *simple* model to your data,
1. simulating data from the posterior predictive distribution of this simple model, and
1. use the simulated data with the real data to fit the model you want to fit.

Operationally things work a bit different than this. 
For one thing, how do you decide how much data you simulate?
The more data you simulate, the larger influence that data has
(and therefore the *simple* model used has) on the posterior. 
This issue can be ameliorated by raising the likelihood for the simulated data
to a power in order to reduce the impact of the simulated data. 

## Mathematical formulation

Suppose you are interested in estimating a model whose density is $f(y|\theta)$
where theta is unknown.
Suppose that it is impossible or impractical to estimate $\theta$ directly from
the data $y = (y_1,\ldots,y_n)$. 
If exists a *simpler* model $g(y|\psi)$ such that you can estimate $\psi$ 
from $y$, then you obtain a posterior for $\psi$ (or a point estimate for $\psi$)
and you simulate from the posterior (point) predictive distribution for 
$g(\tilde{y}|y)$. 

To reduce Monte Carlo variability, you would like to simulate a lot of data. 
To avoid these data having too much influence on the posterior, 
you can weight the likelihood from the simulated data by raising it to a power. 
The power chosen by the authors is $\tau/M$ where $M$ is the sample size of 
the simulated data. 

So the posterior is then 
$$p(\theta|y) \propto \left[ \prod_{i=1}^M f(\tilde{y}_i|\theta) \right]^{\tau/M}
\prod_{i=1}^N f(y_i|\theta).$$


## Binomial model

The authors run directly to GLMs, but perhaps it is instructive to think about
how a catalytic prior works in a binomial model. 
So suppose $Y\sim Bin(n,\theta)$ with unknown $\theta$. 
We generate $\tilde{Y} \sim Bin(m,\psi)$ for some value $\psi$
(it wouldn't be *simpler* if we didn't use a plug-in value). 
The posterior is 
$$
p(\theta|y) \propto \theta^{\tilde{y}\tau/m}(1-\theta)^{(m-\tilde{y})\tau/m}\theta^{y}(1-\theta)^{n-y} =
\theta^{\hat{p}_{\tilde{y}}\tau+y}(1-\theta)^{(1-\hat{p}_{\tilde{y}})\tau + n-y}
$$
where $\hat{p}_{\tilde{y}} = \tilde{y}/m$.

The posterior is then $\theta|y \sim Be(\hat{p}_{\tilde{y}}\tau+y, (1-\hat{p}_{\tilde{y}})\tau + n-y)$.
The *population catalytic prior* replaces $\hat{p}_\tilde{y}$ with $\psi$. 

It is clear from this formulation that $\tau=1$ allows the catalytic prior to
be worth the equivalent of 1 observation. 

In this example, the data actually had no role in constructing the prior. 

## Normal model

Assume $Y_i \stackrel{ind}{\sim} N(\mu,\sigma^2)$.
To construct a simpler model, 
we can set either $\mu$ or $\sigma^2$ to some value. 
As our simpler model, let's use $N(\mu,1)$. 
Assuming an improper uniform prior on $\mu$, we have a posterior predictive
distribution based on $y$ of $N(\overline{y}, 1/n+1)$. 
So, we simulate $\tilde{y}_i \stackrel{ind}{\sim} N(\overline{y}, 1/n+1)$
for $i=1,\ldots,n$. 

Our posterior is then 
$$\begin{array}{rl}
p(\theta|y) &\propto \left[\prod_{i=1}^m (\sigma^2)^{-1/2} \exp\left(-\frac{1}{2\sigma^2}(\tilde{y}_i-\mu)^2\right)\right]^{\tau/m}
\left[\prod_{i=1}^m (\sigma^2)^{-1/2} \exp\left(-\frac{1}{2\sigma^2}(y_i-\mu)^2\right)\right] \\
&\propto (\sigma^2)^{-(\tau+n)/2}\exp(-\frac{1}{2\sigma^2}([\tau + n]\mu^2-2\mu[\tau\overline{\tilde{y}}+n\overline{y}]))
\exp\left(-\frac{1}{2\sigma^2}\left[\frac{\tau}{m}\sum_{i=1}^m (\tilde{y}_i - \overline{\tilde{y}}_i)^2 + \sum_{i=1}^n (y_i-\overline{y})^2 \right]\right)
\end{array}.$$

Thus, $\mu|y,\sigma^2 \sim N\left(\frac{\tau\overline{\tilde{y}}+n\overline{y}]}{\tau+n}, \sigma^2/[\tau+n]\right)$ 
and $\sigma^2|y \sim IG([\tau+n]/2, \frac{\tau}{m}\sum_{i=1}^m (\tilde{y}_i - \overline{\tilde{y}}_i)^2 + (n-1)s^2)$ 
where $s^2$ is the sample variance. 
As $m\to\infty$ we have $\mu|y,\sigma^2 \sim N\left(\overline{y}, \sigma^2/[\tau+n]\right)$ and
$\sigma^2|y \sim IG([\tau+n]/2, \tau 1 + (n-1)s^2)$
(or something pretty similar).
